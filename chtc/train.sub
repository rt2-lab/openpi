# ==============================================================================
# HTCondor submit file for OpenPI training on CHTC GPU Lab
# ==============================================================================
#
# Usage:
#   1. Stage your LeRobot dataset to /staging/<netid>/collab_dataset.tar.gz
#   2. Push your Docker image:  docker push <user>/openpi_train:latest
#   3. Submit:  condor_submit chtc/train.sub
#
# To override defaults from the command line:
#   condor_submit chtc/train.sub config_name=pi05_collab exp_name=run2
#
# References:
#   - CHTC GPU jobs:  https://chtc.cs.wisc.edu/uw-research-computing/gpu-jobs
#   - CHTC Docker:    https://chtc.cs.wisc.edu/uw-research-computing/docker-jobs
#   - CHTC staging:   https://chtc.cs.wisc.edu/uw-research-computing/file-avail-largedata
# ==============================================================================

# --- Container setup ---
# CHTC uses "universe = container" with nvidia-docker support.
# Replace <dockerhub_user> with your DockerHub username.
universe                = container
container_image         = docker://lexu27/openpi_train:latest

# Your CHTC NetID (used for /staging paths). Override at submit-time if desired:
#   condor_submit chtc/train.sub netid=lpxu
netid                   = lpxu

# --- Executable ---
executable              = chtc/run_train.sh

# --- Arguments ---
# These can be overridden at submit time:
#   condor_submit train.sub config_name=pi0_collab exp_name=my_exp
config_name             = pi05_collab
exp_name                = chtc_train_01
arguments               = $(config_name) $(exp_name)

# --- File transfer ---
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT

# Secure W&B auth:
# - Do NOT put WANDB_API_KEY in this file.
# - Instead, export it in your submit shell (or ~/.bashrc) and let HTCondor pass it through.
getenv                  = True

# Input: LeRobot dataset from /staging (adjust path for your netid)
# The dataset tarball will be extracted inside the job by run_train.sh.
# Also transfer any small config overrides from /home if needed.
transfer_input_files    = file:///staging/$(netid)/collab_dataset.tar.gz

# Output: checkpoints tarball written by run_train.sh → back to /staging
transfer_output_files   = checkpoints_out.tar.gz
transfer_output_remaps  = "checkpoints_out.tar.gz = file:///staging/$(netid)/checkpoints_$(Cluster).tar.gz"

# --- Logging ---
log                     = logs/train_$(Cluster).log
output                  = logs/train_$(Cluster).out
error                   = logs/train_$(Cluster).err

# --- Resource requests ---
# Pi0 LoRA fine-tuning fits in ~24 GB VRAM.
# Full fine-tuning needs >=70 GB → request A100-80GB or H100.
# Adjust these based on your config (LoRA vs full).
request_gpus            = 1
request_cpus            = 8
request_memory          = 64GB
request_disk            = 100GB

# Require A100-80GB or better (capability >= 8.0, VRAM >= 40 GB)
gpus_minimum_capability = 8.0
gpus_minimum_memory     = 72000  # Default to megabytes

# --- GPU Lab policies ---
+WantGPULab             = true
# "long" = up to 7 days, max 4 GPUs per user
+GPUJobLength           = "long"

# Enable checkpointing / backfill on research-group GPUs for extra capacity
+is_resumable           = false 

# Require /staging access
Requirements            = (Target.HasCHTCStaging == true)

# --- Environment ---
# Let JAX use 90% of GPU memory
environment             = "XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 HF_HOME=$_CONDOR_SCRATCH_DIR/.cache/hf OPENPI_DATA_HOME=$_CONDOR_SCRATCH_DIR/.cache/openpi WANDB_DIR=$_CONDOR_SCRATCH_DIR/.cache/wandb"

# Stream logs in real time for monitoring
stream_output           = True
stream_error            = True

queue 1
