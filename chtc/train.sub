# ==============================================================================
# HTCondor submit file for OpenPI training on CHTC GPU Lab
# ==============================================================================
#
# Usage:
#   1. Stage your LeRobot dataset to /staging/groups/hagenow_group/collab_dataset.tar.gz
#   2. Push your Docker image:  docker push <user>/openpi_train:latest
#   3. Submit:  condor_submit chtc/train.sub
#
# To override defaults from the command line:
#   condor_submit chtc/train.sub config_name=pi05_collab exp_name=run2
#
# References:
#   - CHTC GPU jobs:  https://chtc.cs.wisc.edu/uw-research-computing/gpu-jobs
#   - CHTC Docker:    https://chtc.cs.wisc.edu/uw-research-computing/docker-jobs
#   - CHTC staging:   https://chtc.cs.wisc.edu/uw-research-computing/file-avail-largedata
# ==============================================================================

# --- Container setup ---
# CHTC uses "universe = container" with nvidia-docker support.
# Replace <dockerhub_user> with your DockerHub username.
universe                = container
container_image         = docker://lexu27/openpi_train:handover_derisk

# Your CHTC NetID (used for login/auth only). Override at submit-time if desired:
#   condor_submit chtc/train.sub netid=lpxu
netid                   = lpxu

# --- Executable ---
executable              = chtc/run_train.sh

# --- Arguments ---
# These can be overridden at submit time:
#   condor_submit train.sub config_name=pi0_collab exp_name=my_exp
config_name             = pi05_handover_derisk
# Default to unique run name per cluster so each submission writes to a new
# staging directory (no collision with previous runs).
exp_name                = pi05_$(Cluster)
arguments               = $(config_name) $(exp_name) $(netid)

# --- File transfer ---
should_transfer_files   = YES
when_to_transfer_output = ON_EXIT_OR_EVICT

# Secure W&B auth:
# - Do NOT put WANDB_API_KEY in this file.
# - Instead, export it in your submit shell (or ~/.bashrc) and let HTCondor pass it through.
getenv                  = True

# Input: LeRobot dataset from shared group staging.
# The dataset tarball will be extracted inside the job by run_train.sh.
# Also transfer any small config overrides from /home if needed.
transfer_input_files    = file:///staging/groups/hagenow_group/collab_dataset.tar.gz

# Transfer one checkpoint bundle file.
# run_train.sh creates checkpoint_bundle.tar after training and on TERM/EXIT.
# This avoids directory URL remap edge-cases for large checkpoint trees.
transfer_output_files   = checkpoint_bundle.tar
transfer_output_remaps  = "checkpoint_bundle.tar = file:///staging/groups/hagenow_group/openpi/$(exp_name).tar"

# --- Logging ---
log                     = logs/train_$(Cluster).log
output                  = logs/train_$(Cluster).out
error                   = logs/train_$(Cluster).err

# --- Resource requests ---
# Pi0 LoRA fine-tuning fits in ~24 GB VRAM.
# Full fine-tuning needs >=70 GB â†’ request A100-80GB or H100.
# Adjust these based on your config (LoRA vs full).
request_gpus            = 1
request_cpus            = 8
request_memory          = 200GB
#
# You got evicted at ~221 GB DiskUsage with request_disk=200GB.
# Give yourself headroom for: extracted dataset + JAX compilation cache + checkpoints.
request_disk            = 1000GB 

# Target H100/H200 only (both are compute capability 9.0 on CHTC).
gpus_minimum_capability = 9.0
# H100 is 80GB; H200 is 141GB.
gpus_minimum_memory     = 80000

# --- GPU Lab policies ---
+WantGPULab             = true
# "long" = up to 7 days, max 4 GPUs per user
+GPUJobLength           = "long"

# Enable checkpointing / backfill on research-group GPUs for extra capacity
+is_resumable           = false 

# Require /staging access
Requirements            = (Target.HasCHTCStaging == true) 

# --- Environment ---
# Let JAX use 90% of GPU memory
environment             = "HOME=$_CONDOR_SCRATCH_DIR XDG_CACHE_HOME=$_CONDOR_SCRATCH_DIR/.cache UV_CACHE_DIR=$_CONDOR_SCRATCH_DIR/.cache/uv XLA_PYTHON_CLIENT_MEM_FRACTION=0.9 HF_HOME=$_CONDOR_SCRATCH_DIR/.cache/hf OPENPI_DATA_HOME=$_CONDOR_SCRATCH_DIR/.cache/openpi WANDB_DIR=$_CONDOR_SCRATCH_DIR/.cache/wandb"

# Stream logs in real time for monitoring
stream_output           = True
stream_error            = True

queue 1